{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zxliCpXL9eE",
        "outputId": "baf8a908-eab7-4e88-e803-243cd8a4ff46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=0d70184d81fb2d912094cdc466fb5a1f6467a110d654d2c123b383930950de70\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pPfnUfFL_aD",
        "outputId": "b36dbeac-3adb-45d4-d105-e4617c860042"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sparkxgb\n",
            "  Downloading sparkxgb-0.1.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyspark==3.1.1 (from sparkxgb)\n",
            "  Downloading pyspark-3.1.1.tar.gz (212.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.3/212.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9 (from pyspark==3.1.1->sparkxgb)\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.6/198.6 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sparkxgb, pyspark\n",
            "  Building wheel for sparkxgb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sparkxgb: filename=sparkxgb-0.1-py3-none-any.whl size=5629 sha256=8c46aa23d8595d137ad4b0b6c0d7f37a54f92c141afe55c1759e3ac672366dd4\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0c/a1/786408e13056fabeb8a72134e101b1e142fc95905c7b0e2a71\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.1-py2.py3-none-any.whl size=212767583 sha256=bb055671a4a9dff74ef2a6618af4a5af009af9c15f384f07f0b090a6d232dcd3\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/3f/72/8efd988f9ae041f051c75e6834cd92dd6d13a726e206e8b6f3\n",
            "Successfully built sparkxgb pyspark\n",
            "Installing collected packages: py4j, pyspark, sparkxgb\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.5.0\n",
            "    Uninstalling pyspark-3.5.0:\n",
            "      Successfully uninstalled pyspark-3.5.0\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.1 sparkxgb-0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install sparkxgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVgIgeYRL6a2",
        "outputId": "6a6bdbae-b52c-48e1-8779-1283cf7a6e34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RandomForestRegressor - Root Mean Squared Error (RMSE): 17.955249125537758, R2: 0.790520508179235\n",
            "LinearRegression - Root Mean Squared Error (RMSE): 14.245497626168719, R2: 0.8681397512953777\n",
            "GBTRegressor - Root Mean Squared Error (RMSE): 16.77583648018136, R2: 0.8171365076661519\n",
            "Polynomial Linear Regression - Root Mean Squared Error (RMSE): 10.069900164650715, R2: 0.934111598570097\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import FloatType\n",
        "from pyspark.sql.functions import col, isnan\n",
        "from pyspark.ml.feature import VectorAssembler, PolynomialExpansion\n",
        "from pyspark.ml.regression import RandomForestRegressor, LinearRegression, GBTRegressor\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.feature import Imputer\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.sql.functions import col, dayofyear\n",
        "from pyspark.sql.functions import concat_ws, col, dayofyear\n",
        "\n",
        "#-------\n",
        "from pyspark.ml.feature import ChiSqSelector\n",
        "from pyspark.ml import PipelineModel\n",
        "\n",
        "\n",
        "\n",
        "# Define a function to save the model to Google Drive\n",
        "def save_model_to_drive(model, model_name):\n",
        "    # Specify the path to the folder where you want to save the model\n",
        "    model_path = f\"/content/drive/MyDrive/SENG550/models/{model_name}\"\n",
        "    model.write().overwrite().save(model_path)\n",
        "\n",
        "\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"FlightDelayPred\").getOrCreate()\n",
        "\n",
        "# Load the data\n",
        "data_path = \"/content/drive/MyDrive/SENG550/2007.csv\"\n",
        "df1 = spark.read.csv(data_path, header=True, inferSchema=True)\n",
        "airportDf = spark.read.csv(\"/content/drive/MyDrive/SENG550/airports.csv\", header=True, inferSchema=True)\n",
        "\n",
        "\n",
        "df2 = df1.select(\"Year\", \"Month\", \"DayofMonth\", \"DayOfWeek\", \"DepTime\", \"CRSDepTime\", \"ArrTime\", \"ArrDelay\", \"DepDelay\", \"CRSArrTime\", \"Distance\", \"Cancelled\", \"Origin\", \"Dest\")\n",
        "\n",
        "df2 = df2.join(airportDf, df2[\"Origin\"] == airportDf[\"iata\"], \"left\")\\\n",
        "    .withColumnRenamed(\"lat\", \"OriginLat\") \\\n",
        "    .withColumnRenamed(\"long\", \"OriginLong\")\n",
        "\n",
        "df2 = df2.drop(\"iata\", \"airport\", \"city\", \"state\", \"country\", \"lat\", \"long\")\n",
        "\n",
        "df2 = df2.join(airportDf, df2[\"Dest\"] == airportDf[\"iata\"], \"left\")\\\n",
        "    .withColumnRenamed(\"lat\", \"DestLat\") \\\n",
        "    .withColumnRenamed(\"long\", \"DestLong\")\n",
        "\n",
        "df2 = df2.drop(\"iata\", \"airport\", \"city\", \"state\", \"country\", \"lat\", \"long\", \"Origin\", \"Dest\")\n",
        "\n",
        "# Remove rows with null or NaN values in target column\n",
        "df2 = df2.filter(df2.ArrDelay.isNotNull() & (~isnan(df2.ArrDelay)) & (df2.ArrDelay != \"NULL\"))\n",
        "df2 = df2.filter(df2[\"Cancelled\"] == 0)\n",
        "df2 = df2.drop(\"Cancelled\")\n",
        "\n",
        "df = df2\\\n",
        "    .withColumn(\"ArrTime\", df2[\"ArrTime\"].cast(FloatType()))\\\n",
        "    .withColumn(\"DepDelay\", df2[\"DepDelay\"].cast(FloatType()))\\\n",
        "    .withColumn(\"DepTime\", df2[\"DepTime\"].cast(FloatType()))\\\n",
        "    .withColumn(\"ArrDelay\", df2[\"ArrDelay\"].cast(FloatType()))\\\n",
        "    .withColumn(\"Distance\", df2[\"Distance\"].cast(FloatType()))\n",
        "\n",
        "\n",
        "df = df.withColumn(\"DayOfYear\", dayofyear(concat_ws(\"-\", col(\"Year\"), col(\"Month\"), col(\"DayofMonth\"))))\n",
        "\n",
        "# Split the data into training and test sets (e.g., 75% training and 25% testing)\n",
        "train_df, test_df = df.randomSplit([0.75, 0.25], seed=42)\n",
        "\n",
        "# Selecting the features and target variable\n",
        "features = [\"Year\", \"Month\", \"DayofMonth\", \"DayOfWeek\", \"DepTime\", \"CRSDepTime\", \"ArrTime\",\n",
        "            \"DepDelay\", \"CRSArrTime\", \"Distance\", \"OriginLat\", \"OriginLong\", \"DestLat\", \"DestLong\"]\n",
        "target = 'ArrDelay'\n",
        "\n",
        "# Imputer replaces missing values in feature columns with their respective mean values\n",
        "imputer = Imputer(\n",
        "    inputCols = features,\n",
        "    strategy = 'mean'\n",
        ")\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "#------ EDA: Basic Statistical Summaries\n",
        "# Display the schema to understand the data types\n",
        "df.printSchema()\n",
        "\n",
        "# Show summary statistics for your dataframe\n",
        "df.describe().show()\n",
        "#------\n",
        "\n",
        "#------ EDA: Distribution of Numeric Features\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Convert Spark DataFrame to Pandas DataFrame for visualization\n",
        "pdf = df.select('DepTime', 'Distance', 'ArrDelay').toPandas()\n",
        "\n",
        "# Plotting histograms\n",
        "plt.figure(figsize=(12, 5))\n",
        "for i, col in enumerate(pdf.columns):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    sns.histplot(pdf[col], kde=True)\n",
        "    plt.title(f'Distribution of {col}')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "#------\n",
        "\n",
        "\n",
        "#------ EDA: Correlation Matrix\n",
        "# Compute correlation matrix\n",
        "corr_matrix = pdf.corr()\n",
        "\n",
        "# Plotting heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()\n",
        "#------\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# ---------\n",
        "# Use Chi-Squared Selector to select a subset of features before polynomial expansion\n",
        "selector = ChiSqSelector(numTopFeatures=10, featuresCol=\"features\", outputCol=\"selectedFeatures\", labelCol=target)\n",
        "# ---------\n",
        "\n",
        "\n",
        "\n",
        "# VectorAssembler to combine feature columns into a single vector column\n",
        "assembler = VectorAssembler(inputCols=features, outputCol=\"features\", handleInvalid=\"skip\")\n",
        "\n",
        "# StandardScaler to standardize features (turns everything to the scale of -3 to +3)\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)\n",
        "\n",
        "# Update your models to use the scaled features\n",
        "rf = RandomForestRegressor(featuresCol=\"scaledFeatures\", labelCol=target)\n",
        "lr = LinearRegression(featuresCol=\"scaledFeatures\", labelCol=target)\n",
        "gbt = GBTRegressor(featuresCol=\"scaledFeatures\", labelCol=target)\n",
        "\n",
        "# Polynomial Expansion for degree 5\n",
        "# polyExpansion = PolynomialExpansion(degree=5, inputCol=\"scaledFeatures\", outputCol=\"polyFeatures\")\n",
        "polyExpansion = PolynomialExpansion(degree=3, inputCol=\"scaledFeatures\", outputCol=\"polyFeatures\")\n",
        "poly_lr = LinearRegression(featuresCol=\"polyFeatures\", labelCol=target)\n",
        "\n",
        "#-------- Define the GBT model with reduced complexity\n",
        "gbt = GBTRegressor(featuresCol=\"scaledFeatures\", labelCol=target, maxIter=10, maxDepth=5)\n",
        "#--------\n",
        "\n",
        "# Update Pipelines to include scaler\n",
        "rf_pipeline = Pipeline(stages=[assembler, scaler, rf])\n",
        "lr_pipeline = Pipeline(stages=[assembler, scaler, lr])\n",
        "gbt_pipeline = Pipeline(stages=[assembler, scaler, gbt])\n",
        "# gbt = GBTRegressor(featuresCol=\"scaledFeatures\", labelCol=target, maxIter=10, maxDepth=5)\n",
        "# poly_pipeline = Pipeline(stages=[assembler, scaler, polyExpansion, poly_lr])\n",
        "poly_pipeline = Pipeline(stages=[assembler, scaler, selector, polyExpansion, poly_lr])\n",
        "\n",
        "\n",
        "# List of models to train and evaluate\n",
        "models = [rf_pipeline,lr_pipeline,gbt_pipeline,poly_pipeline]\n",
        "\n",
        "for model in models:\n",
        "\n",
        "    # Print the RMSE and R2\n",
        "    if model == poly_pipeline:\n",
        "        model_name = \"Polynomial Linear Regression\"\n",
        "    else:\n",
        "        model_name = model.getStages()[-1].__class__.__name__\n",
        "\n",
        "    # Train the model on the training set\n",
        "    trained_model = model.fit(train_df)\n",
        "\n",
        "    # Save the trained model to Google Drive\n",
        "    save_model_to_drive(trained_model, model_name)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    predictions = trained_model.transform(test_df)\n",
        "\n",
        "    # Evaluate the model for RMSE\n",
        "    rmse_evaluator = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "    rmse = rmse_evaluator.evaluate(predictions)\n",
        "\n",
        "    # Evaluate the model for R2\n",
        "    r2_evaluator = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"r2\")\n",
        "    r2 = r2_evaluator.evaluate(predictions)\n",
        "\n",
        "    # # Print the RMSE and R2\n",
        "    # if model == poly_pipeline:\n",
        "    #     model_name = \"Polynomial Linear Regression\"\n",
        "    # else:\n",
        "    #     model_name = model.getStages()[-1].__class__.__name__\n",
        "    print(f\"{model_name} - Root Mean Squared Error (RMSE): {rmse}, R2: {r2}\")\n",
        "\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PohpalOPCkd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}