{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCZEor8Gd3vK",
        "outputId": "d9384eb6-ceaa-4fee-c567-627c1c84f7d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=61c01ebf377b4888b6f207c83a8b4ba30b049557e968c849eae7cc04f6d70a8b\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sparkxgb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GE9bVpejd8Bu",
        "outputId": "86c8d1a1-fe11-42ca-b30b-5cf8930aed58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sparkxgb\n",
            "  Downloading sparkxgb-0.1.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyspark==3.1.1 (from sparkxgb)\n",
            "  Downloading pyspark-3.1.1.tar.gz (212.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.3/212.3 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9 (from pyspark==3.1.1->sparkxgb)\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.6/198.6 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sparkxgb, pyspark\n",
            "  Building wheel for sparkxgb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sparkxgb: filename=sparkxgb-0.1-py3-none-any.whl size=5629 sha256=4dbe328c2466e0a889c42b89442f4bc20280ad312b53d816597d2a3f78f624ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0c/a1/786408e13056fabeb8a72134e101b1e142fc95905c7b0e2a71\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.1-py2.py3-none-any.whl size=212767583 sha256=b368c04242fa4216b5aeddcd439709ef9b8f9a72d647d4609b44fc82a3ddb2e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/3f/72/8efd988f9ae041f051c75e6834cd92dd6d13a726e206e8b6f3\n",
            "Successfully built sparkxgb pyspark\n",
            "Installing collected packages: py4j, pyspark, sparkxgb\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.5.0\n",
            "    Uninstalling pyspark-3.5.0:\n",
            "      Successfully uninstalled pyspark-3.5.0\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.1 sparkxgb-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import FloatType\n",
        "from pyspark.sql.functions import col, isnan\n",
        "from pyspark.ml.feature import VectorAssembler, PolynomialExpansion\n",
        "from pyspark.ml.regression import RandomForestRegressor, LinearRegression, GBTRegressor\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.feature import Imputer\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"FlightDelayPred\").getOrCreate()\n",
        "\n",
        "# Load the data\n",
        "data_path = \"/content/drive/MyDrive/Datasets/1987.csv\"  # Replace with your file path\n",
        "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
        "\n",
        "# Data type casting and handling missing values\n",
        "df = df.withColumn(\"DepTime\", col(\"DepTime\").cast(FloatType()))\n",
        "df = df.withColumn(\"Distance\", col(\"Distance\").cast(FloatType()))\n",
        "df = df.withColumn(\"CRSDepTime\", col(\"CRSDepTime\").cast(FloatType()))\n",
        "df = df.withColumn(\"Month\", col(\"Month\").cast(FloatType()))\n",
        "df = df.withColumn(\"ArrDelay\", col(\"ArrDelay\").cast(FloatType()))\n",
        "df = df.withColumn(\"DepDelay\", col(\"DepDelay\").cast(FloatType()))\n",
        "df = df.withColumn(\"CRSElapsedTime\", col(\"CRSElapsedTime\").cast(FloatType()))\n",
        "df = df.withColumn(\"DayofMonth\", col(\"DayofMonth\").cast(FloatType()))\n",
        "df = df.withColumn(\"FlightNum\", col(\"FlightNum\").cast(FloatType()))\n",
        "\n",
        "# Remove rows with null or NaN values in target column\n",
        "df = df.filter(df.ArrDelay.isNotNull() & (~isnan(df.ArrDelay)))\n",
        "\n",
        "# Selecting the features and target variable\n",
        "features = ['Month', 'CRSDepTime', 'DepTime', 'Distance', 'DepDelay', 'CRSElapsedTime','DayofMonth','FlightNum']\n",
        "target = 'ArrDelay'\n",
        "\n",
        "imputer = Imputer(\n",
        "    inputCols = features,\n",
        "    strategy = 'median'\n",
        ")\n",
        "\n",
        "# VectorAssembler to combine feature columns into a single vector column\n",
        "assembler = VectorAssembler(inputCols=features, outputCol=\"features\", handleInvalid=\"skip\")\n",
        "\n",
        "# Polynomial Expansion for degree 5\n",
        "polyExpansion = PolynomialExpansion(degree=5, inputCol=\"features\", outputCol=\"polyFeatures\")\n",
        "\n",
        "# Define Linear Regression model for polynomial regression\n",
        "poly_lr = LinearRegression(featuresCol=\"polyFeatures\", labelCol=target)\n",
        "\n",
        "# Update pipeline for Polynomial Regression\n",
        "poly_pipeline = Pipeline(stages=[assembler, polyExpansion, poly_lr])\n",
        "\n",
        "# Define the models\n",
        "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=target)\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=target)\n",
        "gbt = GBTRegressor(featuresCol=\"features\", labelCol=target)\n",
        "\n",
        "# Pipelines for the models\n",
        "rf_pipeline = Pipeline(stages=[assembler, rf])\n",
        "lr_pipeline = Pipeline(stages=[assembler, lr])\n",
        "gbt_pipeline = Pipeline(stages=[assembler, gbt])\n",
        "\n",
        "#models = [rf_pipeline, lr_pipeline, gbt_pipeline, poly_pipeline]\n",
        "models = [rf_pipeline]\n",
        "\n",
        "for model in models:\n",
        "    # Train the model\n",
        "    trained_model = model.fit(df)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = trained_model.transform(df)\n",
        "\n",
        "    # Evaluate the model for RMSE\n",
        "    rmse_evaluator = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "    rmse = rmse_evaluator.evaluate(predictions)\n",
        "\n",
        "    # Evaluate the model for R2\n",
        "    r2_evaluator = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"r2\")\n",
        "    r2 = r2_evaluator.evaluate(predictions)\n",
        "\n",
        "    # Print the RMSE and R2\n",
        "    if isinstance(model.getStages()[-1], LinearRegression) and len(model.getStages()) > 2:\n",
        "        model_name = \"Polynomial Linear Regression\"\n",
        "    else:\n",
        "        model_name = model.getStages()[-1].__class__.__name__\n",
        "    print(f\"{model_name} - Root Mean Squared Error (RMSE): {rmse}, R2: {r2}\")\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40JEm53wd8EO",
        "outputId": "29d58d28-bc0d-4351-ccfb-b9544efb411d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestRegressor - Root Mean Squared Error (RMSE): 16.817192711743353, R2: 0.5754737491427673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import FloatType\n",
        "from pyspark.sql.functions import col, isnan\n",
        "from pyspark.ml.feature import VectorAssembler, PolynomialExpansion\n",
        "from pyspark.ml.regression import RandomForestRegressor, LinearRegression, GBTRegressor\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.feature import Imputer\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"FlightDelayPred\").getOrCreate()\n",
        "\n",
        "# Load the data\n",
        "data_path = \"/content/drive/MyDrive/SENG550/2008.csv\"  # Replace with your file path\n",
        "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
        "\n",
        "# Data type casting and handling missing values\n",
        "df = df.withColumn(\"DepTime\", col(\"DepTime\").cast(FloatType()))\n",
        "df = df.withColumn(\"Distance\", col(\"Distance\").cast(FloatType()))\n",
        "df = df.withColumn(\"CRSDepTime\", col(\"CRSDepTime\").cast(FloatType()))\n",
        "df = df.withColumn(\"Month\", col(\"Month\").cast(FloatType()))\n",
        "df = df.withColumn(\"ArrDelay\", col(\"ArrDelay\").cast(FloatType()))\n",
        "df = df.withColumn(\"DepDelay\", col(\"DepDelay\").cast(FloatType()))\n",
        "df = df.withColumn(\"CRSElapsedTime\", col(\"CRSElapsedTime\").cast(FloatType()))\n",
        "df = df.withColumn(\"DayofMonth\", col(\"DayofMonth\").cast(FloatType()))\n",
        "df = df.withColumn(\"FlightNum\", col(\"FlightNum\").cast(FloatType()))\n",
        "\n",
        "# Remove rows with null or NaN values in target column\n",
        "df = df.filter(df.ArrDelay.isNotNull() & (~isnan(df.ArrDelay)))\n",
        "\n",
        "# Split the data into training and test sets (e.g., 80% training and 20% testing)\n",
        "train_df, test_df = df.randomSplit([0.75, 0.25], seed=42)\n",
        "\n",
        "# Selecting the features and target variable\n",
        "features = ['Month', 'CRSDepTime', 'DepTime', 'Distance', 'DepDelay', 'CRSElapsedTime', 'DayofMonth', 'FlightNum']\n",
        "target = 'ArrDelay'\n",
        "\n",
        "imputer = Imputer(\n",
        "    inputCols = features,\n",
        "    strategy = 'mean'\n",
        ")\n",
        "\n",
        "# VectorAssembler to combine feature columns into a single vector column\n",
        "assembler = VectorAssembler(inputCols=features, outputCol=\"features\", handleInvalid=\"skip\")\n",
        "\n",
        "\n",
        "# Polynomial Expansion for degree 5\n",
        "polyExpansion = PolynomialExpansion(degree=5, inputCol=\"features\", outputCol=\"polyFeatures\")\n",
        "# Define Linear Regression model for polynomial regression\n",
        "poly_lr = LinearRegression(featuresCol=\"polyFeatures\", labelCol=target)\n",
        "# Update pipeline for Polynomial Regression\n",
        "poly_pipeline = Pipeline(stages=[assembler, polyExpansion, poly_lr])\n",
        "\n",
        "\n",
        "# Define the models\n",
        "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=target)\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=target)\n",
        "gbt = GBTRegressor(featuresCol=\"features\", labelCol=target)\n",
        "\n",
        "# Pipelines for the models\n",
        "rf_pipeline = Pipeline(stages=[assembler, rf])\n",
        "lr_pipeline = Pipeline(stages=[assembler, lr])\n",
        "gbt_pipeline = Pipeline(stages=[assembler, gbt])\n",
        "\n",
        "# List of models to train and evaluate\n",
        "models = [rf_pipeline,lr_pipeline,gbt_pipeline,poly_pipeline]\n",
        "\n",
        "for model in models:\n",
        "    # Train the model on the training set\n",
        "    trained_model = model.fit(train_df)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    predictions = trained_model.transform(test_df)\n",
        "\n",
        "    # Evaluate the model for RMSE\n",
        "    rmse_evaluator = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "    rmse = rmse_evaluator.evaluate(predictions)\n",
        "\n",
        "    # Evaluate the model for R2\n",
        "    r2_evaluator = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"r2\")\n",
        "    r2 = r2_evaluator.evaluate(predictions)\n",
        "\n",
        "    # Print the RMSE and R2\n",
        "    model_name = model.getStages()[-1].__class__.__name__\n",
        "    print(f\"{model_name} - Root Mean Squared Error (RMSE): {rmse}, R2: {r2}\")\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "LolR7PGie_4p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69e9a6a9-da32-451a-b8f3-9002a6e447c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestRegressor - Root Mean Squared Error (RMSE): 20.4463633834094, R2: 0.734799143742128\n",
            "LinearRegression - Root Mean Squared Error (RMSE): 14.094089967195988, R2: 0.8739865648412362\n",
            "GBTRegressor - Root Mean Squared Error (RMSE): 15.991982332136542, R2: 0.8377639460490582\n",
            "LinearRegression - Root Mean Squared Error (RMSE): 14.072926181899728, R2: 0.8743647260362339\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TDHt3BsSj2TC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}